{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1\n",
    "\n",
    "This notebook is intended to be used as a starting point for your experiments. The instructions can be found in the instructions file located under spec/coursework1.pdf. The methods provided here are just helper functions. If you want more complex graphs such as side by side comparisons of different experiments you should learn more about matplotlib and implement them. Before each experiment remember to re-initialize neural network weights and reset the data providers so you get a properly initialized experiment. For each experiment try to keep most hyperparameters the same except the one under investigation so you can understand what the effects of each are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True):\n",
    "    \n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors, notebook=notebook)\n",
    "\n",
    "    # Run the optimiser for 5 epochs (full passes through the training set)\n",
    "    # printing statistics every epoch.\n",
    "    stats, keys, run_time = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    \n",
    "    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code will set up the data providers, random number\n",
    "# generator and logger objects needed for training runs. As\n",
    "# loading the data from file take a little while you generally\n",
    "# will probably not want to reload the data providers on\n",
    "# every training run. If you wish to reset their state you\n",
    "# should instead use the .reset() method of the data providers.\n",
    "import numpy as np\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider, EMNISTDataProvider\n",
    "\n",
    "# Seed a random number generator\n",
    "seed = 11102019 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 100\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)\n",
    "test_data = EMNISTDataProvider('test', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function to save and view results\n",
    "import pickle\n",
    "def save_data(data,file_name):\n",
    "    with open(file_name,'wb') as f:\n",
    "        pickle.dump(data,f)\n",
    "    print('save successfully')\n",
    "def load_data(file_name):\n",
    "    with open(file_name,'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print('load successfully')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required library\n",
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer, DropoutLayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.learning_rules import AdamLearningRule\n",
    "from mlp.optimisers import Optimiser\n",
    "from mlp.penalties import L1Penalty\n",
    "from mlp.penalties import L2Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GlorotUniformInit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0b05ba960583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m47\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mweights_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlorotUniformInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mbiases_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConstantInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GlorotUniformInit' is not defined"
     ]
    }
   ],
   "source": [
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 100\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "# train and store the simple model.\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    ReluLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "error = CrossEntropySoftmaxError()\n",
    "# Use a basic gradient descent learning rule\n",
    "learning_rule = AdamLearningRule()\n",
    "\n",
    "#Remember to use notebook=False when you write a script to be run in a terminal\n",
    "_ = train_model_and_plot_stats(\n",
    "    model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook=True)\n",
    "\n",
    "fig_1.savefig('plots/baseline(Err).pdf')\n",
    "fig_2.savefig('plots/baseline(Acc).pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 32, 64, 128 ReLU hidden units\n",
    "# 100 epoch\n",
    "# AdamLearningRule default learning rate: 1e-03\n",
    "# One hidden layer\n",
    "\n",
    "#setup hyperparameters\n",
    "# Uncomment below to train for different learning_rate\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim = 784, 47\n",
    "hidden_dims = [32, 64, 128]\n",
    "\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "units_all_stats =[]\n",
    "\n",
    "for hidden_dim in hidden_dims:\n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "    model = MultipleLayerModel([\n",
    "        AffineLayer(input_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "        ReluLayer(),\n",
    "        AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "    ])\n",
    "\n",
    "    error = CrossEntropySoftmaxError()\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = AdamLearningRule()\n",
    "\n",
    "    #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook = True)\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])\n",
    "    units_all_stats.append(stats)\n",
    "    \n",
    "    # Add figure title\n",
    "    fig_1.suptitle('Error for network with {} hidden units'.format(hidden_dim))\n",
    "    fig_2.suptitle('Accuracy for network {} hidden units'.format(hidden_dim))\n",
    "    # Save figure as Pdf\n",
    "    fig_1.savefig('plots/{}_hidden_units(Err).pdf'.format(hidden_dim))\n",
    "    fig_2.savefig('plots/{}_hidden_units(Acc).pdf'.format(hidden_dim))\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "j = 0\n",
    "for hidden_dim in hidden_dims:\n",
    "    results = {'Number of ReLU hidden units':hidden_dim,\n",
    "               'Number of epochs':num_epochs,\n",
    "               'Defualt learning_rate': 1e-3,\n",
    "               'Number of Hidden units':hidden_dim,\n",
    "               'final error(train)':final_errors_train[j],\n",
    "               'final error(valid)':final_errors_valid[j],\n",
    "               'final acc(train)':final_accs_train[j],\n",
    "               'final acc(valid)':final_accs_valid[j]\n",
    "              }\n",
    "    save_data(results,'stats/{0:3d}_hidden_units.npy'.format(j+1))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(units_all_stats,'stats/units.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 128 ReLU hidden units\n",
    "# 100 epoch\n",
    "# AdamLearningRule default learning rate: 1e-03\n",
    "# Hidden layers: 1,2,3\n",
    "\n",
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "layers_all_stats = []\n",
    "\n",
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)\n",
    "\n",
    "# Define three model with different number of hidden layers : 1,2,3\n",
    "models = [MultipleLayerModel([\n",
    "            AffineLayer(input_dim, hidden_dim, weights_init, biases_init),  # input layer\n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # hidden layer 1\n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, output_dim, weights_init, biases_init)]), # output layer\n",
    "         MultipleLayerModel([\n",
    "            AffineLayer(input_dim, hidden_dim, weights_init, biases_init), # input layer\n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # hidden layer 1\n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # hidden layer 2\n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, output_dim, weights_init, biases_init)]), # output layer\n",
    "         MultipleLayerModel([\n",
    "            AffineLayer(input_dim, hidden_dim, weights_init, biases_init),  # input layer\n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # hidden layer 1\n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # hidden layer 2\n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # hidden layer 3\n",
    "            ReluLayer(),\n",
    "            AffineLayer(hidden_dim, output_dim, weights_init, biases_init)]) # output layer\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    error = CrossEntropySoftmaxError()\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = AdamLearningRule()\n",
    "\n",
    "    #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook = True)\n",
    "    \n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])\n",
    "    layers_all_stats.append(stats)\n",
    "    \n",
    "    # Add figure title\n",
    "    fig_1.suptitle('Error for network with {} hidden layers'.format(models.index(model)+1))\n",
    "    fig_2.suptitle('Accuracy for network with{} hidden layes'.format(models.index(model)+1))\n",
    "    # Save figure as Pdf\n",
    "    fig_1.savefig('plots/{}_hidden_layers(Err).pdf'.format(models.index(model)+1))\n",
    "    fig_2.savefig('plots/{}_hidden_layers(Acc).pdf'.format(models.index(model)+1))\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "j = 0\n",
    "for model in models:\n",
    "    results = {'Number of ReLU hidden units':hidden_dim,\n",
    "               'Number of epochs':num_epochs,\n",
    "               'Defualt learning_rate': 1e-3,\n",
    "               'Number of Hidden layers':j+1,\n",
    "               'final error(train)':final_errors_train[j],\n",
    "               'final error(valid)':final_errors_valid[j],\n",
    "               'final acc(train)':final_accs_train[j],\n",
    "               'final acc(valid)':final_accs_valid[j]\n",
    "              }\n",
    "    save_data(results,'stats/{0:3d}_hidden_layers.npy'.format(j+1))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(layers_all_stats,'stats/layers.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 128 ReLU hidden units\n",
    "# 100 epoch\n",
    "# AdamLearningRule default learning rate: 1e-03\n",
    "# L1Penalty coefficients [0.0001, 0.0001/2, 0.00001]\n",
    "\n",
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "L1Penalty_coefficients = [0.0001, 0.0001/2, 0.00001]\n",
    "\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "L1_all_stats = []\n",
    "\n",
    "for L1Penalty_coefficient in L1Penalty_coefficients:\n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "    model = MultipleLayerModel([\n",
    "                AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weights_penalty=L1Penalty(L1Penalty_coefficient)),  # input layer\n",
    "                ReluLayer(),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L1Penalty(L1Penalty_coefficient)), # hidden layer 1\n",
    "                ReluLayer(),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L1Penalty(L1Penalty_coefficient)), # hidden layer 2\n",
    "                ReluLayer(),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L1Penalty(L1Penalty_coefficient)), # hidden layer 3\n",
    "                ReluLayer(),\n",
    "                AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weights_penalty=L1Penalty(L1Penalty_coefficient))])# output layer\n",
    "\n",
    "\n",
    "    error = CrossEntropySoftmaxError()\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = AdamLearningRule()\n",
    "\n",
    "    #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook = True)\n",
    "    \n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])\n",
    "    L1_all_stats.append(stats)\n",
    "\n",
    "    # Add figure title\n",
    "    fig_1.suptitle('Error: 128 units, 3 hidden layers, {} L1penalty coef'.format(L1Penalty_coefficient))\n",
    "    fig_2.suptitle('Accuracy: 128 units, 3 hidden layers, {} L1penalty coef'.format(L1Penalty_coefficient))\n",
    "    # Save figure as Pdf\n",
    "    fig_1.savefig('plots/L1_{0:.0e}(Err).pdf'.format(L1Penalty_coefficient))\n",
    "    fig_2.savefig('plots/L1_{0:.0e}(Acc).pdf'.format(L1Penalty_coefficient))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# save data\n",
    "j = 0\n",
    "for L1Penalty_coefficient in L1Penalty_coefficients:\n",
    "    results = {'Number of ReLU hidden units':hidden_dim,\n",
    "               'Number of epochs':num_epochs,\n",
    "               'Defualt learning_rate': 1e-3,\n",
    "               'L1Penalty coef':L1Penalty_coefficient,\n",
    "               'final error(train)':final_errors_train[j],\n",
    "               'final error(valid)':final_errors_valid[j],\n",
    "               'final acc(train)':final_accs_train[j],\n",
    "               'final acc(valid)':final_accs_valid[j]\n",
    "              }\n",
    "    save_data(results,'stats/L1Penalty_{}.npy'.format(L1Penalty_coefficient))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(L1_all_stats,'stats/L1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 128 ReLU hidden units\n",
    "# 100 epoch\n",
    "# AdamLearningRule default learning rate: 1e-03\n",
    "# L2Penalty coefficient [0.001, 0.001/2, 0.0001, 0.0001/2]\n",
    "\n",
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "L2Penalty_coefficients = [0.001, 0.001/2, 0.0001, 0.0001/2]\n",
    "\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "L2_all_stats = []\n",
    "for L2Penalty_coefficient in L2Penalty_coefficients:\n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "    model = MultipleLayerModel([\n",
    "                AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)),  # input layer\n",
    "                ReluLayer(),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)), # hidden layer 1\n",
    "                ReluLayer(),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)), # hidden layer 2\n",
    "                ReluLayer(),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)), # hidden layer 3\n",
    "                ReluLayer(),\n",
    "                AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient))])# output layer\n",
    "\n",
    "\n",
    "    error = CrossEntropySoftmaxError()\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = AdamLearningRule()\n",
    "\n",
    "    #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook = True)\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])\n",
    "    L2_all_stats.append(stats)\n",
    "\n",
    "    # Add figure title\n",
    "    fig_1.suptitle('Error: 128 units, 3 hidden layers, {} L2penalty coef'.format(L2Penalty_coefficient))\n",
    "    fig_2.suptitle('Accuracy: 128 units, 3 hidden layers, {} L2penalty coef'.format(L2Penalty_coefficient))\n",
    "    # Save figure as Pdf\n",
    "    fig_1.savefig('plots/L2_{0:.0e}(Err).pdf'.format(L2Penalty_coefficient))\n",
    "    fig_2.savefig('plots/L2_{0:.0e}(Acc).pdf'.format(L2Penalty_coefficient))\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# save data\n",
    "j = 0\n",
    "for L2Penalty_coefficient in L2Penalty_coefficients:\n",
    "    results = {'Number of ReLU hidden units':hidden_dim,\n",
    "               'Number of epochs':num_epochs,\n",
    "               'Defualt learning_rate': 1e-3,\n",
    "               'L2Penalty coef':L2Penalty_coefficient,\n",
    "               'final error(train)':final_errors_train[j],\n",
    "               'final error(valid)':final_errors_valid[j],\n",
    "               'final acc(train)':final_accs_train[j],\n",
    "               'final acc(valid)':final_accs_valid[j]\n",
    "              }\n",
    "    save_data(results,'stats/L2Penalty_{}.npy'.format(L2Penalty_coefficient))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(L2_all_stats,'stats/L2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 128 ReLU hidden units\n",
    "# 100 epoch\n",
    "# AdamLearningRule default learning rate: 1e-03\n",
    "# Dropout: inclusion probability [0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "\n",
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "incl_probs = [0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "Dropout_all_stats=[]\n",
    "\n",
    "for incl_prob in incl_probs:\n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "    model = MultipleLayerModel([\n",
    "                AffineLayer(input_dim, hidden_dim, weights_init, biases_init),  # input layer\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # hidden layer 1\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # hidden layer 2\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # hidden layer 3\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, output_dim, weights_init, biases_init)])# output layer\n",
    "\n",
    "\n",
    "    error = CrossEntropySoftmaxError()\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = AdamLearningRule()\n",
    "\n",
    "    #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook = True)\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])\n",
    "    Dropout_all_stats.append(stats)\n",
    "\n",
    "    # Add figure title\n",
    "    fig_1.suptitle('Error: 128 units, 3 hidden layers, Dropout with {} inclusion probability'.format(incl_prob))\n",
    "    fig_2.suptitle('Accuracy: 128 units, 3 hidden layers, Dropout with {} inclusion probability'.format(incl_prob))\n",
    "    # Save figure as Pdf\n",
    "    fig_1.savefig('plots/Dropout_p_{0:.2f}(Err).pdf'.format(incl_prob))\n",
    "    fig_2.savefig('plots/Dropout_p_{0:.2f}(Acc).pdf'.format(incl_prob))\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# save data\n",
    "j = 0\n",
    "for incl_prob in incl_probs:\n",
    "    results = {'Number of ReLU hidden units':hidden_dim,\n",
    "               'Number of epochs':num_epochs,\n",
    "               'Defualt learning_rate': 1e-3,\n",
    "               'Dropout:Inclusion prob':incl_prob,\n",
    "               'final error(train)':final_errors_train[j],\n",
    "               'final error(valid)':final_errors_valid[j],\n",
    "               'final acc(train)':final_accs_train[j],\n",
    "               'final acc(valid)':final_accs_valid[j]\n",
    "              }\n",
    "    save_data(results,'stats/Dropout_p_{0:.2f}.npy'.format(incl_prob))\n",
    "    load_data('stats/Dropout_p_{0:.2f}.npy'.format(incl_prob))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(Dropout_all_stats,'stats/Dropout.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 128 ReLU hidden units\n",
    "# 100 epoch\n",
    "# AdamLearningRule default learning rate: 1e-03\n",
    "# Dropout: inclusion probability [0.9]\n",
    "# L1 penalty, 5e-05(OR 1e-05)\n",
    "\n",
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "incl_probs = [0.9]\n",
    "L1Penalty_coefficient = 5e-05\n",
    "\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "p90_L1_all_stats = []\n",
    "for incl_prob in incl_probs:\n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "    model = MultipleLayerModel([\n",
    "                AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weights_penalty=L1Penalty(L1Penalty_coefficient)),  # input layer\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L1Penalty(L1Penalty_coefficient)), # hidden layer 1\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L1Penalty(L1Penalty_coefficient)), # hidden layer 2\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L1Penalty(L1Penalty_coefficient)), # hidden layer 3\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weights_penalty=L1Penalty(L1Penalty_coefficient))])# output layer\n",
    "\n",
    "\n",
    "    error = CrossEntropySoftmaxError()\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = AdamLearningRule()\n",
    "\n",
    "    #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook = True)\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])\n",
    "    p90_L1_all_stats.append(stats)\n",
    "\n",
    "    # Add figure title\n",
    "    fig_1.suptitle('Error:Dropout with {} inclusion probability, L1 penalty constant {}'.format(incl_prob,L1Penalty_coefficient))\n",
    "    fig_2.suptitle('Accuracy:Dropout with {} inclusion probability, L1 penalty constant {}'.format(incl_prob,L1Penalty_coefficient))\n",
    "    # Save figure as Pdf\n",
    "    fig_1.savefig('plots/Dropout_p_{0:.2f}_L1{1:.0e}(Err).pdf'.format(incl_prob,L1Penalty_coefficient))\n",
    "    fig_2.savefig('plots/Dropout_p_{0:.2f}_L1{1:.0e}(Acc).pdf'.format(incl_prob,L1Penalty_coefficient))\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# save data\n",
    "j = 0\n",
    "for incl_prob in incl_probs:\n",
    "    results = {'Number of ReLU hidden units':hidden_dim,\n",
    "               'Number of epochs':num_epochs,\n",
    "               'Defualt learning_rate': 1e-3,\n",
    "               'Dropout:Inclusion prob':incl_prob,\n",
    "               'Weight penalty: L1':L1Penalty_coefficient,\n",
    "               'final error(train)':final_errors_train[j],\n",
    "               'final error(valid)':final_errors_valid[j],\n",
    "               'final acc(train)':final_accs_train[j],\n",
    "               'final acc(valid)':final_accs_valid[j]\n",
    "              }\n",
    "    save_data(results,'stats/Dropout_p_{0:.2f}_L1{1:.0e}.npy'.format(incl_prob,L1Penalty_coefficient))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(p90_L1_all_stats,'stats/Dropout_p_{0:.2f}_L1{1:.0e}.npy'.format(incl_prob,L1Penalty_coefficient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 128 ReLU hidden units\n",
    "# 100 epoch\n",
    "# AdamLearningRule default learning rate: 1e-03\n",
    "# Dropout: inclusion probability [0.9]\n",
    "# L2 penalty, 1e-03/2(OR 1e-04)\n",
    "\n",
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "incl_probs = [0.9]\n",
    "L2Penalty_coefficient = 1e-03/2\n",
    "\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "p90_L2_all_stats = []\n",
    "for incl_prob in incl_probs:\n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "    model = MultipleLayerModel([\n",
    "                AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)),  # input layer\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)), # hidden layer 1\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)), # hidden layer 2\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)), # hidden layer 3\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient))])# output layer\n",
    "\n",
    "\n",
    "    error = CrossEntropySoftmaxError()\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = AdamLearningRule()\n",
    "\n",
    "    #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook = True)\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])\n",
    "    p90_L2_all_stats.append(stats)\n",
    "\n",
    "    # Add figure title\n",
    "    fig_1.suptitle('Error:Dropout with {} inclusion probability, L2 penalty constant {}'.format(incl_prob,L2Penalty_coefficient))\n",
    "    fig_2.suptitle('Accuracy:Dropout with {} inclusion probability, L2 penalty constant {}'.format(incl_prob,L2Penalty_coefficient))\n",
    "    # Save figure as Pdf\n",
    "    fig_1.savefig('plots/Dropout_p_{0:.2f}_L2{1:.0e}(Err).pdf'.format(incl_prob,L2Penalty_coefficient))\n",
    "    fig_2.savefig('plots/Dropout_p_{0:.2f}_L2{1:.0e}(Acc).pdf'.format(incl_prob,L2Penalty_coefficient))\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# save data\n",
    "j = 0\n",
    "for incl_prob in incl_probs:\n",
    "    results = {'Number of ReLU hidden units':hidden_dim,\n",
    "               'Number of epochs':num_epochs,\n",
    "               'Defualt learning_rate': 1e-3,\n",
    "               'Dropout:Inclusion prob':incl_prob,\n",
    "               'Weight penalty: L2':L2Penalty_coefficient,\n",
    "               'final error(train)':final_errors_train[j],\n",
    "               'final error(valid)':final_errors_valid[j],\n",
    "               'final acc(train)':final_accs_train[j],\n",
    "               'final acc(valid)':final_accs_valid[j]\n",
    "              }\n",
    "    save_data(results,'stats/Dropout_p_{0:.2f}_L2{1:.0e}.npy'.format(incl_prob,L2Penalty_coefficient))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(p90_L2_all_stats,'stats/Dropout_p_{0:.2f}_L2{1:.0e}.npy'.format(incl_prob,L2Penalty_coefficient))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 128 ReLU hidden units\n",
    "# 100 epoch\n",
    "# AdamLearningRule default learning rate: 1e-03/2\n",
    "# Dropout: inclusion probability [0.9]\n",
    "# L2 penalty, 1e-03/2(OR 1e-04)\n",
    "\n",
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "incl_probs = [0.9]\n",
    "L2Penalty_coefficient = 1e-04\n",
    "learning_rate = 1e-04\n",
    "\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "for incl_prob in incl_probs:\n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "    model = MultipleLayerModel([\n",
    "                AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)),  # input layer\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)), # hidden layer 1\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)), # hidden layer 2\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)), # hidden layer 3\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient))])# output layer\n",
    "\n",
    "\n",
    "    error = CrossEntropySoftmaxError()\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = AdamLearningRule(learning_rate = learning_rate)\n",
    "\n",
    "    #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook = True)\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])\n",
    "\n",
    "    # Add figure title\n",
    "    fig_1.suptitle('Error:Dropout incl_prob = {0}, L2 constant {1:.0e},learning_rate{2:.0e}'.format(incl_prob,L2Penalty_coefficient,learning_rate))\n",
    "    fig_2.suptitle('Accuracy:Dropout incl_prob = {0}, L2 constant {1:.0e}, learning_rate{2:.0e}'.format(incl_prob,L2Penalty_coefficient,learning_rate))\n",
    "    # Save figure as Pdf\n",
    "    fig_1.savefig('plots/Dropout_p_{0:.2f}_L2{1:.0e}_lr{2:.0e}(Err).pdf'.format(incl_prob,L2Penalty_coefficient,learning_rate))\n",
    "    fig_2.savefig('plots/Dropout_p_{0:.2f}_L2{1:.0e}_lr{2:.0e}(Acc).pdf'.format(incl_prob,L2Penalty_coefficient,learning_rate))\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# save data\n",
    "j = 0\n",
    "for incl_prob in incl_probs:\n",
    "    results = {'Number of ReLU hidden units':hidden_dim,\n",
    "               'Number of epochs':num_epochs,\n",
    "               'Learning_rate': learning_rate,\n",
    "               'Dropout:Inclusion prob':incl_prob,\n",
    "               'Weight penalty: L2':L2Penalty_coefficient,\n",
    "               'final error(train)':final_errors_train[j],\n",
    "               'final error(valid)':final_errors_valid[j],\n",
    "               'final acc(train)':final_accs_train[j],\n",
    "               'final acc(valid)':final_accs_valid[j]\n",
    "              }\n",
    "    save_data(results,'stats/Dropout_p_{0:.2f}_L2{1:.0e}_lr{2:.0e}.npy'.format(incl_prob,L2Penalty_coefficient,learning_rate))\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment the suggested setting from Srivastava's paper\n",
    "# 128 ReLU hidden units\n",
    "# 100 epoch\n",
    "# AdamLearningRule default learning rate: 1e-03\n",
    "# Dropout: inclusion probability 0.5 0.7 for hidden layer and 0.8 0.9 for input layer\n",
    "\n",
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "incl_probs = [0.8]\n",
    "\n",
    "final_errors_train = []\n",
    "final_errors_valid = []\n",
    "final_accs_train = []\n",
    "final_accs_valid = []\n",
    "all_stats=[]\n",
    "for incl_prob in incl_probs:\n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    valid_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "    model = MultipleLayerModel([\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob+0.2),\n",
    "                AffineLayer(input_dim, hidden_dim, weights_init, biases_init),  # input layer\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # hidden layer 1\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # hidden layer 2\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), # hidden layer 3\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, output_dim, weights_init, biases_init)])# output layer\n",
    "\n",
    "\n",
    "    error = CrossEntropySoftmaxError()\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = AdamLearningRule()\n",
    "\n",
    "    #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, notebook = True)\n",
    "\n",
    "    final_errors_train.append(stats[-1, keys['error(train)']])\n",
    "    final_errors_valid.append(stats[-1, keys['error(valid)']])\n",
    "    final_accs_train.append(stats[-1, keys['acc(train)']])\n",
    "    final_accs_valid.append(stats[-1, keys['acc(valid)']])\n",
    "    all_stats.append(stats)\n",
    "\n",
    "    fig_1.savefig('plots/Dropout_p_{0:.2f}_1.0input(Err).pdf'.format(incl_prob))\n",
    "    fig_2.savefig('plots/Dropout_p_{0:.2f}_1.0input(Acc).pdf'.format(incl_prob))\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# save data\n",
    "save_data(all_stats,'stats/Dropout_p_{0:.2f}_1.0input.npy'.format(incl_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform test\n",
    "# 128 ReLU hidden units\n",
    "# 100 epoch\n",
    "# AdamLearningRule default learning rate: 1e-03/3\n",
    "# Dropout: inclusion probability [0.9]\n",
    "# L2 penalty 1e-04\n",
    "\n",
    "#setup hyperparameters\n",
    "num_epochs = 100\n",
    "stats_interval = 1\n",
    "input_dim, output_dim, hidden_dim = 784, 47, 128\n",
    "incl_probs = [0.9]\n",
    "L2Penalty_coefficient = 1e-04\n",
    "learning_rate = 1e-03/3\n",
    "\n",
    "test_all_stats =[]\n",
    "\n",
    "for incl_prob in incl_probs:\n",
    "    # Reset random number generator and data provider states on each run\n",
    "    # to ensure reproducibility of results\n",
    "    rng.seed(seed)\n",
    "    train_data.reset()\n",
    "    test_data.reset()\n",
    "\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init = ConstantInit(0.)\n",
    "    model = MultipleLayerModel([\n",
    "                AffineLayer(input_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)),  # input layer\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)), # hidden layer 1\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)), # hidden layer 2\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient)), # hidden layer 3\n",
    "                ReluLayer(),\n",
    "                DropoutLayer(rng = rng, incl_prob = incl_prob),\n",
    "                AffineLayer(hidden_dim, output_dim, weights_init, biases_init, weights_penalty=L2Penalty(L2Penalty_coefficient))])# output layer\n",
    "\n",
    "\n",
    "    error = CrossEntropySoftmaxError()\n",
    "    # Use a basic gradient descent learning rule\n",
    "    learning_rule = AdamLearningRule(learning_rate = learning_rate)\n",
    "\n",
    "    #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    stats, keys, run_time, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, test_data, num_epochs, stats_interval, notebook = True)\n",
    "    test_all_stats.append(stats)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(test_all_stats,'stats/(test)Dropout_p_{0:.2f}_L2{1:.0e}_lr{2:.0e}.npy'.format(incl_prob,L2Penalty_coefficient,learning_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
